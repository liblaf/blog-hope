import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,b as s,o as i}from"./app-BMihMyie.js";const c={};function d(t,e){return i(),a("div",null,[...e[0]||(e[0]=[s(`<h2 id="environment" tabindex="-1"><a class="header-anchor" href="#environment"><span>Environment</span></a></h2><p>P100 GPU 最大支持每个 SM 64KB shared memory，但每个 thread block 最多只支持 48KB</p><h2 id="method" tabindex="-1"><a class="header-anchor" href="#method"><span>Method</span></a></h2><p>使用 <a href="https://lab.cs.tsinghua.edu.cn/hpc/doc/exp/3.apsp/" target="_blank" rel="noopener noreferrer">实验三 - 高性能计算导论实验文档 (tsinghua.edu.cn)</a> 中的分块方法. 一个 thread block 处理一个矩阵分块. 每个 thread block 所需使用的数据全部拷贝到 shared memory 中. 在 <code>threadIdx</code> 的基础上偏移 <code>i_start</code>, <code>j_start</code> 或 <code>center_block_start</code> 即可将 shared memory 中的坐标映射到 global memory 中的不同矩阵分块.</p><h3 id="phase-1" tabindex="-1"><a class="header-anchor" href="#phase-1"><span>Phase 1</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>k = [ p * b, (p + 1) * b )</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>对于每个 thread block, 访问范围包括 <code>k * k</code> 共 <code>b * b</code> 个 <code>int</code>, 也即需要 <code>b * b * sizeof(int)</code> 大小的 shared memory.</p><p>对于每个 <code>p</code>, 仅需一个 thread block 即可完成任务. 但是很浪费.</p><h3 id="phase-2" tabindex="-1"><a class="header-anchor" href="#phase-2"><span>Phase 2</span></a></h3><h4 id="horizontal" tabindex="-1"><a class="header-anchor" href="#horizontal"><span>Horizontal</span></a></h4><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>k = [ p * b          , (p + 1) * b          )</span></span>
<span class="line"><span>i = [ p * b          , (p + 1) * b          )</span></span>
<span class="line"><span>j = [ blockIdx.x * b , (blockIdx.x + 1) * b )</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>特别的, 若 <code>j</code> 的范围恰好在 center block 后, 即 <code>blockIdx.x * b &gt;= center_block_start</code> 时, 则需额外偏移 <code>b</code>.</p><p>对于每个 thread block, 访问范围包括 <code>i * j</code>, <code>i * k</code>, <code>k * j</code>, 其中 <code>i * j</code> 和 <code>k * j</code> 重合, 因此共 <code>2 * b * b</code> 个 <code>int</code>.</p><p>共需 <code>(ceil(n / p) - 1) * 1</code> 个 thread block.</p><h4 id="vertical" tabindex="-1"><a class="header-anchor" href="#vertical"><span>Vertical</span></a></h4><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>k = [ p * b          , (p + 1) * b          )</span></span>
<span class="line"><span>i = [ blockIdx.y * b , (blockIdx.y + 1) * b )</span></span>
<span class="line"><span>j = [ p * b          , (p + 1) * b          )</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>特别的, 若 <code>i</code> 的范围恰好在 center block 后, 即 <code>blockIdx.y * b &gt;= center_block_start</code> 时, 则需额外偏移 <code>b</code>.</p><p>对于每个 thread block, 访问范围包括 <code>i x j</code>, <code>i x k</code>, <code>k x j</code>, 其中 <code>i x j</code> 和 <code>i x k</code> 重合, 因此共 <code>2 * b * b</code> 个 <code>int</code>.</p><p>共需 <code>1 * (ceil(n / p) - 1)</code> 个 thread block.</p><h3 id="phase-3" tabindex="-1"><a class="header-anchor" href="#phase-3"><span>Phase 3</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>k = [ p * b          , (p + 1) * b          )</span></span>
<span class="line"><span>i = [ blockIdx.y * b , (blockIdx.y + 1) * b )</span></span>
<span class="line"><span>i = [ blockIdx.x * b , (blockIdx.x + 1) * b )</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>特别的, 若 <code>i</code> 或 <code>j</code> 的范围恰好在 center block 后, 即 <code>blockIdx * b &gt;= center_block_start</code> 时, 则需额外偏移 <code>b</code>.</p><p>对于每个 thread block, 访问范围包括 <code>i x j</code>, <code>i x k</code>, <code>k x j</code>, 均不重合, 共 <code>3 * b * b</code> 个 <code>int</code>.</p><p>共需 <code>(ceil(n / p) - 1) * (ceil(n / p) - 1)</code> 个 thread block.</p><p>综合考虑, 取 <code>b = 32</code>, 每个 thread block 共 <code>32 x 32</code> 个 thread, 既不会超出 shared memory 限制, 又能够避免 bank conflict.</p><h2 id="performance" tabindex="-1"><a class="header-anchor" href="#performance"><span>Performance</span></a></h2><table><thead><tr><th>n</th><th><code>apspRef()</code> (ms)</th><th><code>apsp()</code> (ms)</th><th>Speedup</th></tr></thead><tbody><tr><td>1000</td><td>14.814903</td><td>2.969371</td><td>4.98923947</td></tr><tr><td>2500</td><td>377.148402</td><td>37.660415</td><td>10.01445157</td></tr><tr><td>5000</td><td>2972.073596</td><td>260.960028</td><td>11.38899938</td></tr><tr><td>7500</td><td>10016.146987</td><td>872.866804</td><td>11.47500047</td></tr><tr><td>10000</td><td>22632.211686</td><td>2060.573817</td><td>10.98345107</td></tr></tbody></table><p>在 <code>n = 1000</code> 下进行 profiling.</p><h3 id="nvprof-events" tabindex="-1"><a class="header-anchor" href="#nvprof-events"><span><code>nvprof</code> Events</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>Invocations                                Event Name         Min         Max         Avg       Total</span></span>
<span class="line"><span>Device &quot;Tesla P100-PCIE-16GB (0)&quot;</span></span>
<span class="line"><span>    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase2KernelHorizontal(int, int*, int, int)</span></span>
<span class="line"><span>         96                   shared_ld_bank_conflict           0           0           0           0</span></span>
<span class="line"><span>         96                   shared_st_bank_conflict           0           0           0           0</span></span>
<span class="line"><span>    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase2KernelVertical(int, int*, int, int)</span></span>
<span class="line"><span>         96                   shared_ld_bank_conflict           0           0           0           0</span></span>
<span class="line"><span>         96                   shared_st_bank_conflict           0           0           0           0</span></span>
<span class="line"><span>    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase1Kernel(int, int*, int, int)</span></span>
<span class="line"><span>         96                   shared_ld_bank_conflict           0           0           0           0</span></span>
<span class="line"><span>         96                   shared_st_bank_conflict           0           0           0           0</span></span>
<span class="line"><span>    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase3Kernel(int, int*, int, int)</span></span>
<span class="line"><span>         96                   shared_ld_bank_conflict           0           0           0           0</span></span>
<span class="line"><span>         96                   shared_st_bank_conflict           0           0           0           0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>没有出现 bank conflict.</p><h3 id="nvprof-metrics" tabindex="-1"><a class="header-anchor" href="#nvprof-metrics"><span><code>nvprof</code> Metrics</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>Invocations                               Metric Name                         Metric Description         Min         Max         Avg</span></span>
<span class="line"><span>Device &quot;Tesla P100-PCIE-16GB (0)&quot;</span></span>
<span class="line"><span>    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase2KernelHorizontal(int, int*, int, int)</span></span>
<span class="line"><span>         96                         branch_efficiency                          Branch Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                 warp_execution_efficiency                  Warp Execution Efficiency      97.97%     100.00%      98.03%</span></span>
<span class="line"><span>         96         warp_nonpred_execution_efficiency   Warp Non-Predicated Execution Efficiency      81.95%      95.89%      95.26%</span></span>
<span class="line"><span>         96                            gld_efficiency              Global Memory Load Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                            gst_efficiency             Global Memory Store Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                         shared_efficiency                   Shared Memory Efficiency      67.38%      69.64%      67.45%</span></span>
<span class="line"><span>    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase2KernelVertical(int, int*, int, int)</span></span>
<span class="line"><span>         96                         branch_efficiency                          Branch Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                 warp_execution_efficiency                  Warp Execution Efficiency      51.59%     100.00%      98.49%</span></span>
<span class="line"><span>         96         warp_nonpred_execution_efficiency   Warp Non-Predicated Execution Efficiency      41.59%      97.89%      95.93%</span></span>
<span class="line"><span>         96                            gld_efficiency              Global Memory Load Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                            gst_efficiency             Global Memory Store Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                         shared_efficiency                   Shared Memory Efficiency      18.58%      69.01%      67.43%</span></span>
<span class="line"><span>    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase1Kernel(int, int*, int, int)</span></span>
<span class="line"><span>         96                         branch_efficiency                          Branch Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                 warp_execution_efficiency                  Warp Execution Efficiency      47.13%     100.00%      98.35%</span></span>
<span class="line"><span>         96         warp_nonpred_execution_efficiency   Warp Non-Predicated Execution Efficiency      46.31%      97.76%      96.15%</span></span>
<span class="line"><span>         96                            gld_efficiency              Global Memory Load Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                            gst_efficiency             Global Memory Store Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                         shared_efficiency                   Shared Memory Efficiency      18.52%      68.69%      67.12%</span></span>
<span class="line"><span>    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase3Kernel(int, int*, int, int)</span></span>
<span class="line"><span>         96                         branch_efficiency                          Branch Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                 warp_execution_efficiency                  Warp Execution Efficiency      98.12%     100.00%      98.18%</span></span>
<span class="line"><span>         96         warp_nonpred_execution_efficiency   Warp Non-Predicated Execution Efficiency      84.89%      95.93%      95.23%</span></span>
<span class="line"><span>         96                            gld_efficiency              Global Memory Load Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                            gst_efficiency             Global Memory Store Efficiency     100.00%     100.00%     100.00%</span></span>
<span class="line"><span>         96                         shared_efficiency                   Shared Memory Efficiency      67.69%      69.91%      67.75%</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可以看出各项指标的利用率都较充分, 但 shared memory 利用率较低.</p>`,34)])])}const r=n(c,[["render",d]]),o=JSON.parse('{"path":"/2022/course-work/hpc/2022-06-05-pa3-%E5%85%A8%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF.html","title":"PA3: 全源最短路","lang":"en-US","frontmatter":{"category":["Course Work"],"date":"2022-06-05T00:00:00.000Z","modified":"2025-09-20T15:07:39.000Z","tags":["CUDA","Introduction_to_High_Performance_Computing"],"title":"PA3: 全源最短路","description":"Environment P100 GPU 最大支持每个 SM 64KB shared memory，但每个 thread block 最多只支持 48KB Method 使用 实验三 - 高性能计算导论实验文档 (tsinghua.edu.cn) 中的分块方法. 一个 thread block 处理一个矩阵分块. 每个 thread block 所需使...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"PA3: 全源最短路\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-06-05T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"liblaf\\",\\"url\\":\\"https://github.com/liblaf\\"}]}"],["meta",{"property":"og:url","content":"https://hope.liblaf.me/2022/course-work/hpc/2022-06-05-pa3-%E5%85%A8%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF.html"}],["meta",{"property":"og:site_name","content":"liblaf"}],["meta",{"property":"og:title","content":"PA3: 全源最短路"}],["meta",{"property":"og:description","content":"Environment P100 GPU 最大支持每个 SM 64KB shared memory，但每个 thread block 最多只支持 48KB Method 使用 实验三 - 高性能计算导论实验文档 (tsinghua.edu.cn) 中的分块方法. 一个 thread block 处理一个矩阵分块. 每个 thread block 所需使..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"article:tag","content":"Introduction_to_High_Performance_Computing"}],["meta",{"property":"article:tag","content":"CUDA"}],["meta",{"property":"article:published_time","content":"2022-06-05T00:00:00.000Z"}]]},"git":{},"readingTime":{"minutes":3.13,"words":938},"filePathRelative":"2022/course-work/hpc/2022-06-05-pa3-全源最短路.md","excerpt":"<h2>Environment</h2>\\n<p>P100 GPU 最大支持每个 SM 64KB shared memory，但每个 thread block 最多只支持 48KB</p>\\n<h2>Method</h2>\\n<p>使用 <a href=\\"https://lab.cs.tsinghua.edu.cn/hpc/doc/exp/3.apsp/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">实验三 - 高性能计算导论实验文档 (tsinghua.edu.cn)</a> 中的分块方法. 一个 thread block 处理一个矩阵分块. 每个 thread block 所需使用的数据全部拷贝到 shared memory 中. 在 <code>threadIdx</code> 的基础上偏移 <code>i_start</code>, <code>j_start</code> 或 <code>center_block_start</code> 即可将 shared memory 中的坐标映射到 global memory 中的不同矩阵分块.</p>","autoDesc":true}');export{r as comp,o as data};
